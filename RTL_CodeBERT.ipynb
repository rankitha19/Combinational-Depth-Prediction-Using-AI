import torch
import numpy as np
import re
from datasets import load_dataset
from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig, TrainingArguments, Trainer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load MetRex dataset
dataset = load_dataset("scale-lab/MetRex")

# Function to extract combinational depth from delay column
def extract_depth_from_delay(delay_text):
    match = re.search(r"The critical path goes through the following gates: (.*?). Delay of", delay_text)
    if match:
        gates = match.group(1).split(", ")
        return len(gates)  # Number of gates in the critical path
    return None

# Apply extraction
dataset = dataset["train"].map(lambda x: {"combinational_depth": extract_depth_from_delay(x["delay"])})

# Filter out samples where depth couldn't be extracted
dataset = dataset.filter(lambda x: x["combinational_depth"] is not None)

# Convert to LLM fine-tuning format
def format_example(example):
    return {
        "input_text": f"Predict the combinational depth of this RTL code:\n\n{example['RTL']}",
        "target_text": example["combinational_depth"],
    }

dataset = dataset.map(format_example)

# Split dataset into train/test
dataset = dataset.train_test_split(test_size=0.1, seed=42)

# Load CodeBERT tokenizer & model
model_name = "microsoft/codebert-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Tokenize inputs
def tokenize_function(example):
    return tokenizer(example["input_text"], padding="max_length", truncation=True, max_length=512)

tokenized_dataset = dataset.map(tokenize_function, batched=True)

# Convert labels to float for regression
def convert_labels(example):
    example["labels"] = float(example["target_text"])
    return example

tokenized_dataset = tokenized_dataset.map(convert_labels)

# Load model with regression configuration
config = AutoConfig.from_pretrained(model_name, num_labels=1, problem_type="regression")
model = AutoModelForSequenceClassification.from_pretrained(model_name, config=config)

# Freeze all layers except classifier
for param in model.roberta.parameters():
    param.requires_grad = False
model.roberta.eval()

# Training settings
training_args = TrainingArguments(
    output_dir="./checkpoints",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    eval_strategy="epoch",
    save_strategy="epoch",
    save_total_limit=2,
    logging_steps=10,
    learning_rate=1e-4,
    num_train_epochs=3,
    weight_decay=0.01,
    report_to="none",
    push_to_hub=False,
    fp16=torch.cuda.is_available(),
)

# MSE Loss for regression
def compute_metrics(pred):
    preds = pred.predictions.squeeze()
    labels = pred.label_ids.squeeze()
    return {"mse": np.mean((preds - labels) ** 2)}

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
    compute_metrics=compute_metrics,
)

# Train model
trainer.train()

# Save final model
trainer.save_model("./final_model")
